# -*- coding: utf-8 -*-
"""LED-it_fine_tuning_ITACASEHOLD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oAKXWmKkaD99oI6_UAnZL8r5EYIYzYCf
"""

import wandb
#wandb.login()
wandb.login(key="2a596940b79889cecb0b70cd92f2fd7f4f3befa3")

#!pip install accelerate peft bitsandbytes transformers trl quanto
#!pip install evaluate rouge_score nltk

#from google.colab import drive
#drive.mount('/content/drive')

import os
import numpy as np
import json
import re
from pprint import pprint
import evaluate
import nltk
from nltk.tokenize import sent_tokenize
import pandas as pd
import torch
from datasets import Dataset, load_dataset
from huggingface_hub import notebook_login
from peft import (
    LoraConfig,
    PeftModel,
    get_peft_model,
)
from transformers import (
    AutoModelForCausalLM,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    MBart50Tokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    QuantoConfig,
)
from torch.utils.data.dataloader import DataLoader
from accelerate import Accelerator
from trl import SFTTrainer
import matplotlib.pyplot as plt

"""## Setup execution environment"""

if torch.cuda.is_available():
    # Tell PyTorch to use the GPU.
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
    device_map = {"": 0}
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

"""## Setup evaluation  metrics"""

nltk.download('punkt')
rouge_score = evaluate.load('rouge')
bleu_score = evaluate.load('bleu')

"""## Utility functions"""

def generate_text(data_point):
  summary = data_point['doc']
  text = ""
  for item in data_point["summary"]:
    text_clean = clean_text(item)
    text += text_clean.strip()

def clean_text(text):
  text = re.sub(r"http\S+", "", text)
  text = re.sub(r"@[^\s]+", "", text)
  text = re.sub(r"\s+", " ", text)
  return re.sub(r"\^[^ ]+", "", text)

def process_dataset(data: Dataset):
  return(
    data.shuffle(seed=42)
      .map(generate_text)
      .remove_columns(
          [
            "url",
            "title",
            "materia",
         ]
    )
 )

def flatten(nested_list):
    return [' '.join(sublist) for sublist in nested_list]

def preprocess_function(examples, tokenizer, max_source_length, max_target_length):
  # Flatten the documents if necessary
  docs = flatten(examples["doc"])
  inputs = [doc for doc in docs]
  model_inputs = tokenizer(inputs, max_length=max_source_length, truncation=True, padding=True, return_tensors='pt')

  # Flatten the summaries if necessary
  summaries = flatten(examples["summary"])
  labels = tokenizer(summaries, max_length=max_target_length, truncation=True, padding=True, return_tensors='pt')

  model_inputs["labels"] = labels["input_ids"]
  return model_inputs

def three_sentence_summary(text):
  return "\n".join(sent_tokenize(text)[:3])

def evaluate_baseline(dataset, metric):
  summaries = [three_sentence_summary(text) for text in dataset['summary']]
  return metric.compute(predictions=summaries, references=dataset['doc'])

def compute_metrics(eval_pred):
  predictions, labels = eval_pred
  # Decode generated summaries into text
  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
  # Replace -100 in the labels as we can't decode them
  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
  # Decode reference summaries into text
  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

  decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
  decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
  result_rouge = rouge_score.compute(
      predictions=decoded_preds, references=decoded_labels, use_stemmer=True
  )

  result_bleu = bleu_score.compute(predictions=decoded_preds, references=decoded_labels)

  # Extract the median scores
  result_rouge = {key: value * 100 for key, value in result_rouge.items()}
  result_rouge = {k: round(v, 4) for k, v in result_rouge.items()}
  return {"rouge": result_rouge}#, "bleu": result_bleu}

"""# Load dataset"""

# Model from Hugging Face hub
base_model = "allenai/led-base-16384"

# New instruction dataset
dataset_name = "itacasehold/itacasehold"


# Fine-tuned model
output_dir = f"C:\Users\marty\Desktop\BISS_Cineca\models\led\{base_model}.finetuned-itacasehold"

dataset = load_dataset(dataset_name)
dataset

dataset_train = dataset['train'].select(range(200))
dataset_validation = dataset['validation']

dataset_train = process_dataset(dataset_train)
dataset_validation = process_dataset(dataset_validation)

print(f'Sample data after processing: \n{dataset_train[0]}')

"""## Analyze length documentand summaries"""

# Calculate the lengths of the phrases in the training and validation sets
train_lengths_doc = [len(item['doc']) for item in dataset['train']]
train_lengths_summary = [len(item['summary']) for item in dataset['train']]
validation_lengths_doc = [len(item['doc']) for item in dataset['validation']]
validation_lengths_summary = [len(item['summary']) for item in dataset['validation']]

# Create a histogram for each set
plt.figure(figsize=(6, 6))

plt.subplot(2, 2, 1)
plt.hist(train_lengths_doc, bins=30, color='blue')
plt.title('Doc Lengths in the Training Set')
plt.xlabel('Length')
plt.ylabel('Frequency')

plt.subplot(2, 2, 2)
plt.hist(train_lengths_summary, bins=30, color='green')
plt.title('Summary Lengths in the Training Set')
plt.xlabel('Length')
plt.ylabel('Frequency')

plt.subplot(2, 2, 3)
plt.hist(validation_lengths_doc, bins=30, color='red')
plt.title('Doc Lengths in the Validation Set')
plt.xlabel('Length')
plt.ylabel('Frequency')

plt.subplot(2, 2, 4)
plt.hist(validation_lengths_summary, bins=30, color='purple')
plt.title('Summary Lengths in the Validation Set')
plt.xlabel('Length')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""## Tokenization

"""

tokenizer = AutoTokenizer.from_pretrained(base_model)
print(f'Vocabulary size: {tokenizer.vocab_size}')

"""## Data preprocessing

"""

max_source_length = 8192
max_target_length = 512
tokenized_train_dataset = dataset_train.map(
    preprocess_function,
    fn_kwargs={
        "tokenizer": tokenizer,
        "max_source_length": max_source_length,
        "max_target_length": max_target_length
    },
    batched=True,
    remove_columns=["doc", "summary"]
)

tokenized_valid_dataset = dataset_validation.map(
    preprocess_function,
    fn_kwargs={
        "tokenizer": tokenizer,
        "max_source_length": max_source_length,
        "max_target_length": max_target_length
    },
    batched=True,
    remove_columns=["doc", "summary"]
)

"""## Evaluation baseline"""

print(three_sentence_summary(dataset_train[1]['summary']))

score = evaluate_baseline(dataset_validation, rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn] * 100, 2)) for rn in rouge_names)
rouge_dict

score = evaluate_baseline(dataset_validation, bleu_score)
score

"""# Fine-Tuning LED"""

quantization_config = QuantoConfig(weights="int8")

LORA_R = 8
LORA_ALPHA = 16
LORA_DROPOUT= 0.05
LORA_TARGET_MODULES = [
    "q_proj",
    "k_proj",
    "v_proj",
    "o_proj",
]

lora_config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=LORA_TARGET_MODULES,
    lora_dropout=LORA_DROPOUT,
    bias="none",
    task_type="CAUSAL_LM",
)

batch_size = 2
micro_batch_size = 2
num_train_epochs = 5
warmup_ratio = 0.1
logging_steps = 1
gradient_accumulation_steps = batch_size // micro_batch_size

args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=micro_batch_size,
    gradient_accumulation_steps = gradient_accumulation_steps,
    warmup_ratio = warmup_ratio,
    optim = "adamw_bnb_8bit",
    save_total_limit=1,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_strategy="steps",
    logging_steps=logging_steps,
    load_best_model_at_end=True,
    push_to_hub=False,
    fp16=True
)

model = AutoModelForSeq2SeqLM.from_pretrained(
    base_model,
    device_map= device_map,
    #torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    use_cache=False,
    quantization_config=quantization_config,
    gradient_checkpointing=True,
    )

model.add_adapter(lora_config, adapter_name="adapter_1")

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors='pt')

features = [tokenized_train_dataset[i] for i in range(2)]
data_collator(features)

"""### Training"""
wandb.init(
    project="led-train",
    config={
        "learning_rate": 1e-04,
        "architecture": "led",
        "dataset": "ita-casehold",
    }
)


trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_valid_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
model.config.use_cache = False

# speeds up training
if torch.__version__ >= "2":
    model = torch.compile(model)



# Training
for epoch in range(args.num_train_epochs):
    train_result = trainer.train()
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    trainer.save_state()
    print(metrics)

    # Log della training loss su WandB
    wandb.log({"train_loss": metrics["train_loss"]})

wandb.finish()

output_dir = f"C:\Users\marty\Desktop\BISS_Cineca\models\led\"

model.save_pretrained(output_dir)

"""## Performance on validation set"""

wandb.init(
    project="led-val",
    config={
        "learning_rate": "-",
        "architecture": "led",
    }
)

# Valutazione del modello
eval_result = trainer.evaluate()

# Recupera la validation loss dal risultato della valutazione
val_loss = eval_result["eval_loss"]

# Logga la validation loss su WandB
wandb.log({"val_loss": val_loss})

wandb.finish()


"""# Performance on test set"""

dataset_test = dataset['test']
print(len(dataset_test))
dataset_test = process_dataset(dataset_test)
tokenized_test_dataset = dataset_test.map(
    preprocess_function,
    fn_kwargs={
        "tokenizer": tokenizer,
        "max_source_length": max_source_length,
        "max_target_length": max_target_length
    },
    batched=True,
    remove_columns=["doc", "summary"]
)

results = trainer.predict(tokenized_test_dataset)
results_metrics = results.metrics


# Percorso del file di testo in cui verranno salvate le metriche
output_file_path = "C:\Users\marty\Desktop\BISS_Cineca\models\led\metrics.txt"

# Aprire il file di testo in modalità scrittura
with open(output_file_path, "w") as output_file:
    # Iterare sulle metriche e salvarle nel file di testo
    for metric_name, metric_value in results_metrics.items():
        output_file.write(f"{metric_name}: {metric_value}\n")
        
"""## Predition example"""

from transformers import pipeline

pipe = pipeline("text2text-generation", model=output_dir, tokenizer=tokenizer)

print(dataset_test[0]["doc"][:max_source_length])

result = pipe(dataset_test[0]["doc"][:max_source_length])
result